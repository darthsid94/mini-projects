{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Statistics Ib - Frequentism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second Frequentist inference mini-project! Over the course of working on this mini-project and the previous frequentist mini-project, you'll learn the fundamental concepts associated with frequentist inference. The following list includes the topics you will become familiar with as you work through these two mini-projects:\n",
    "* the _z_-statistic\n",
    "* the _t_-statistic\n",
    "* the difference and relationship between the two\n",
    "* the Central Limit Theorem, its assumptions and consequences\n",
    "* how to estimate the population mean and standard deviation from a sample\n",
    "* the concept of a sampling distribution of a test statistic, particularly for the mean\n",
    "* how to combine these concepts to calculate confidence intervals and p-values\n",
    "* how those confidence intervals and p-values allow you to perform hypothesis (or A/B) tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what a random variable is\n",
    "* what a probability density function (pdf) is\n",
    "* what the cumulative density function is\n",
    "* a high-level sense of what the Normal distribution\n",
    "\n",
    "If these concepts are new to you, please take a few moments to Google these topics in order to get a sense of what they are and how you might use them.\n",
    "\n",
    "These two notebooks were designed to bridge the gap between having a basic understanding of probability and random variables and being able to apply these concepts in Python. This second frequentist inference mini-project focuses on a real-world application of this type of inference to give you further practice using these concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used only data from a known normal distribution. You'll now tackle real data, rather than simulated data, and answer some relevant real-world business problems using the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital medical charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a hospital has hired you as their data analyst. An administrator is working on the hospital's business operations plan and needs you to help them answer some business questions. This mini-project, as well as the bootstrap and Bayesian inference mini-projects also found in this unit are designed to illustrate how each of the inferential statistics methods have their uses for different use cases. In this assignment notebook, you're going to use frequentist statistical inference on a data sample to answer the questions:\n",
    "* has the hospital's revenue stream fallen below a key threshold?\n",
    "* are patients with insurance really charged different amounts than those without?\n",
    "Answering that last question with a frequentist approach makes some assumptions, or requires some knowledge, about the two groups. In the next mini-project, you'll use bootstrapping to test that assumption. And in the final mini-project of the unit, you're going to create a model for simulating _individual_ charges (not a sampling distribution) that the hospital can use to model a range of scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some data on medical charges obtained from [Kaggle](https://www.kaggle.com/easonlai/sample-insurance-claim-prediction-dataset). For the purposes of this exercise, assume the observations are the result of random sampling from our one hospital. Recall in the previous assignment, we introduced the Central Limit Theorem (CLT), and how it tells us that the distributions of sample statistics approach a normal distribution as $n$ increases. The amazing thing about this is that it applies to the sampling distributions of statistics that have been calculated from even highly non-normal distributions of data. Remember, also, that hypothesis testing is very much based on making inferences about such sample statistics. You're going to rely heavily on the CLT to apply frequentist (parametric) tests to answer the questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from numpy.random import seed\n",
    "medical = pd.read_csv('data/insurance2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>insuranceclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex     bmi  children  smoker  region      charges  insuranceclaim\n",
       "0   19    0  27.900         0       1       3  16884.92400               1\n",
       "1   18    1  33.770         1       0       2   1725.55230               1\n",
       "2   28    1  33.000         3       0       2   4449.46200               0\n",
       "3   33    1  22.705         0       0       1  21984.47061               0\n",
       "4   32    1  28.880         0       0       1   3866.85520               1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Plot the histogram of charges and calculate the mean and standard deviation. Comment on the appropriateness of these statistics for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green>The distribution is heavily skewed with a long tail on the right. The mean of charges is 13270.422 with a standard deviation of 12105.48 which depicts high variability. Even though this distribution is higly skewed, since the sample size is large enough(more than 30), we can rely on CLT which states that the sampling distribution of mean charges would be get closer to normal as the sample size increases.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "medical.charges.plot(kind='hist',bins=10)\n",
    "plt.xlabel('charges')\n",
    "plt.title('Frequency distribution of charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the charges from the sample 13270.4223\n",
      "Standard deviation of the charges from the sample 12105.485\n"
     ]
    }
   ],
   "source": [
    "mean=round(np.mean(medical.charges),4)\n",
    "std=round(np.std(medical.charges),4)\n",
    "print('Mean of the charges from the sample', mean)\n",
    "print('Standard deviation of the charges from the sample', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ The administrator is concerned that the actual average charge has fallen below 12000, threatening the hospital's operational model. On the assumption that these data represent a random sample of charges, how would you justify that these data allow you to answer that question? And what would be the most appropriate frequentist test, of the ones discussed so far, to apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green> Since the given data represent a random sample of charges, maybe the avg. charges observed here are because of this specific sample that we picked from the population data. If we take multiple random samples from the population data, we will observe different avg. charges. So, here we can conduct a significance test using t distribution(since we don't know the population parameters) for the observed data. Under this, we can calculate how likely it is to observe the given mean or more extreme data to ascertain if we this observed figure has been observed just the chance, under the assumption that the true average charges is 12000. We will do this by evaluating and interpreting the p value under the null hypothesis that the true mean is indeed 12000. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Given the nature of the administrator's concern, what is the appropriate confidence interval in this case? A one-sided or two-sided interval? Calculate the critical value and the relevant 95% confidence interval for the mean and comment on whether the administrator should be concerned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green>This will be a one sided interval since we are interested in whether the population parameter is BELOW a certain value. There will be a lower bound which will be calculated while the upper bound would be infinity </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample size\n",
    "n=len(medical.charges)\n",
    "#bessel corrected standard deviation\n",
    "std_dev=np.std(medical.charges,ddof=1)\n",
    "#estimated standard error\n",
    "standard_error=std_dev/np.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t score for 95% confidence level for given sample is: -1.6459941145571317\n",
      "Margin of error: -544.9351\n"
     ]
    }
   ],
   "source": [
    "#confidence level\n",
    "x=0.95\n",
    "alpha=1-x\n",
    "#t statistic\n",
    "t_score= t.ppf(alpha,n-1)\n",
    "print('t score for 95% confidence level for given sample is:', t_score)\n",
    "margin_of_error=round(t_score*standard_error,4)\n",
    "print('Margin of error:', margin_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval using t statistic: (12725.4872, 'infinity')\n"
     ]
    }
   ],
   "source": [
    "lower_bound=mean+margin_of_error\n",
    "#upper bound would be infinity since this is a sided confindence interval with a lower bound\n",
    "print('Confidence Interval using t statistic:', (lower_bound,'infinity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green> Since the lower bound itself of the confidence interval is greater than 12000, the administrator's concerns should be allayed. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The administrator then wants to know whether people with insurance really are charged a different amount to those without.\n",
    "\n",
    "__Q:__ State the null and alternative hypothesis here. Use the _t_-test for the difference between means where the pooled standard deviation of the two groups is given by\n",
    "\\begin{equation}\n",
    "s_p = \\sqrt{\\frac{(n_0 - 1)s^2_0 + (n_1 - 1)s^2_1}{n_0 + n_1 - 2}}\n",
    "\\end{equation}\n",
    "\n",
    "and the *t* test statistic is then given by\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\bar{x}_0 - \\bar{x}_1}{s_p \\sqrt{1/n_0 + 1/n_1}}.\n",
    "\\end{equation}\n",
    "\n",
    "What assumption about the variances of the two groups are we making here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green>Null Hypothesis: The difference of the mean charges for people with insurance and those without insurance is zero.<br>Alternative Hypothesis: The difference of the mean charges for people with insurance and those without insurance is not zero.<br> Under the pooled approach, it is assumed that the variance of the charges for people with insurance and that for those without insurance are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Perform this hypothesis test both manually, using the above formulae, and then using the appropriate function from [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-tests) (hint, you're looking for a function to perform a _t_-test on two independent samples). For the manual approach, calculate the value of the test statistic and then its probability (the p-value). Verify you get the same results from both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green> As the calculations below show, the results obtained from both approaches are same</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green> We are making the following assumptions for the two samples(with insurance and without insurance)<br>1. The two samples are randomly drawn from the population and independent of each other.<br>2. Data in both samples are normally distributed. If that is not the case, we can rely on CLT since the sample sizes of both are sufficiently large(mpre than 30)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample size of people with insurance\n",
    "n_0=len(medical[medical['insuranceclaim']==1])\n",
    "#sample size of people without insurance\n",
    "n_1=len(medical[medical['insuranceclaim']==0])\n",
    "#creating dataframe of required data \n",
    "grouped=medical.groupby('insuranceclaim')['charges'].agg([np.mean,np.std])\n",
    "#mean charges for people with insurance\n",
    "x_0=grouped.loc[1,'mean']\n",
    "#mean charges for people without insurance\n",
    "x_1=grouped.loc[0,'mean']\n",
    "#standard deviation of charges for people with insurance\n",
    "s_0=grouped.loc[1,'std']\n",
    "#standard deviation of charges for people without insurance\n",
    "s_1=grouped.loc[0,'std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t statistic is: 11.89329903087671\n"
     ]
    }
   ],
   "source": [
    "#calculating pooled sample standard deviation\n",
    "s_p= np.sqrt(((n_0-1)*(s_0**2)+(n_1-1)*(s_1**2))/(n_0+n_1-2))\n",
    "#calculating t statistic\n",
    "t_statistic=(x_0-x_1)/(s_p*np.sqrt((1/n_0)+(1/n_1)))\n",
    "print('t statistic is:', t_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manually calculated p value 0.0\n"
     ]
    }
   ],
   "source": [
    "#degrees of freedom\n",
    "df=n_0+n_1-2\n",
    "#p value\n",
    "p = (1.0 - t.cdf((t_statistic), df)) * 2.0\n",
    "print('manually calculated p value',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using inbuilt functions: Ttest_indResult(statistic=11.893299030876712, pvalue=4.461230231620717e-31)\n"
     ]
    }
   ],
   "source": [
    "with_insurance_claim=medical[medical['insuranceclaim']==1].charges\n",
    "without_insurance_claim=medical[medical['insuranceclaim']==0].charges\n",
    "print('Results using inbuilt functions:',stats.ttest_ind(with_insurance_claim,without_insurance_claim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green> The p value is very small, almost 0. If we are testing our hypothesis at a significance level of 0.05, we would reject the null hypothesis based on this p-value. This means that there may indeed be significant difference in the charges for those with insurance claims and for those without insurance claims.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Hopefully you got the exact same numerical results. This shows that you correctly calculated the numbers by hand. Secondly, you used the correct function and saw that it's much easier to use. All you need to do pass your data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ In the above calculations, we assumed the sample variances were equal. We may well suspect they are not (we'll explore this in another assignment). The calculation becomes a little more complicated to do by hand in this case, but we now know of a helpful function. Check the documentation for the function to tell it not to assume equal variances and perform the test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Calculate the T-test for the means of *two independent* samples of scores.\n",
      "\n",
      "    This is a two-sided test for the null hypothesis that 2 independent samples\n",
      "    have identical average (expected) values. This test assumes that the\n",
      "    populations have identical variances by default.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a, b : array_like\n",
      "        The arrays must have the same shape, except in the dimension\n",
      "        corresponding to `axis` (the first, by default).\n",
      "    axis : int or None, optional\n",
      "        Axis along which to compute test. If None, compute over the whole\n",
      "        arrays, `a`, and `b`.\n",
      "    equal_var : bool, optional\n",
      "        If True (default), perform a standard independent 2 sample test\n",
      "        that assumes equal population variances [1]_.\n",
      "        If False, perform Welch's t-test, which does not assume equal\n",
      "        population variance [2]_.\n",
      "\n",
      "        .. versionadded:: 0.11.0\n",
      "    nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "        Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "        'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "        values. Default is 'propagate'.\n",
      "\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float or array\n",
      "        The calculated t-statistic.\n",
      "    pvalue : float or array\n",
      "        The two-tailed p-value.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    We can use this test, if we observe two independent samples from\n",
      "    the same or different population, e.g. exam scores of boys and\n",
      "    girls or of two ethnic groups. The test measures whether the\n",
      "    average (expected) value differs significantly across samples. If\n",
      "    we observe a large p-value, for example larger than 0.05 or 0.1,\n",
      "    then we cannot reject the null hypothesis of identical average scores.\n",
      "    If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,\n",
      "    then we reject the null hypothesis of equal averages.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "\n",
      "    .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy import stats\n",
      "    >>> np.random.seed(12345678)\n",
      "\n",
      "    Test with sample with identical means:\n",
      "\n",
      "    >>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "    >>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "    >>> stats.ttest_ind(rvs1,rvs2)\n",
      "    (0.26833823296239279, 0.78849443369564776)\n",
      "    >>> stats.ttest_ind(rvs1,rvs2, equal_var = False)\n",
      "    (0.26833823296239279, 0.78849452749500748)\n",
      "\n",
      "    `ttest_ind` underestimates p for unequal variances:\n",
      "\n",
      "    >>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\n",
      "    >>> stats.ttest_ind(rvs1, rvs3)\n",
      "    (-0.46580283298287162, 0.64145827413436174)\n",
      "    >>> stats.ttest_ind(rvs1, rvs3, equal_var = False)\n",
      "    (-0.46580283298287162, 0.64149646246569292)\n",
      "\n",
      "    When n1 != n2, the equal variance t-statistic is no longer equal to the\n",
      "    unequal variance t-statistic:\n",
      "\n",
      "    >>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\n",
      "    >>> stats.ttest_ind(rvs1, rvs4)\n",
      "    (-0.99882539442782481, 0.3182832709103896)\n",
      "    >>> stats.ttest_ind(rvs1, rvs4, equal_var = False)\n",
      "    (-0.69712570584654099, 0.48716927725402048)\n",
      "\n",
      "    T-test with different means, variance, and n:\n",
      "\n",
      "    >>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\n",
      "    >>> stats.ttest_ind(rvs1, rvs5)\n",
      "    (-1.4679669854490653, 0.14263895620529152)\n",
      "    >>> stats.ttest_ind(rvs1, rvs5, equal_var = False)\n",
      "    (-0.94365973617132992, 0.34744170334794122)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(stats.ttest_ind.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green> As shown in the documentation above, we can set the argument equal_var=False to drop the assumption that the variances of the two samples are equal</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without eqaul variances assumption: Ttest_indResult(statistic=13.298031957975649, pvalue=1.1105103216309125e-37)\n"
     ]
    }
   ],
   "source": [
    "print('Results without eqaul variances assumption:', stats.ttest_ind(with_insurance_claim,without_insurance_claim, equal_var=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Conceptual question: look through the documentation for statistical test functions in scipy.stats. You'll see the above _t_-test for a sample, but can you see an equivalent one for performing a *z*-test from a sample? Comment on your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ <font color=green>There doesn't appear to be an equivalent function for performing z-test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Statistical functions (:mod:`scipy.stats`)\n",
      "==========================================\n",
      "\n",
      ".. currentmodule:: scipy.stats\n",
      "\n",
      "This module contains a large number of probability distributions as\n",
      "well as a growing library of statistical functions.\n",
      "\n",
      "Each univariate distribution is an instance of a subclass of `rv_continuous`\n",
      "(`rv_discrete` for discrete distributions):\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   rv_continuous\n",
      "   rv_discrete\n",
      "   rv_histogram\n",
      "\n",
      "Continuous distributions\n",
      "========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   alpha             -- Alpha\n",
      "   anglit            -- Anglit\n",
      "   arcsine           -- Arcsine\n",
      "   argus             -- Argus\n",
      "   beta              -- Beta\n",
      "   betaprime         -- Beta Prime\n",
      "   bradford          -- Bradford\n",
      "   burr              -- Burr (Type III)\n",
      "   burr12            -- Burr (Type XII)\n",
      "   cauchy            -- Cauchy\n",
      "   chi               -- Chi\n",
      "   chi2              -- Chi-squared\n",
      "   cosine            -- Cosine\n",
      "   crystalball       -- Crystalball\n",
      "   dgamma            -- Double Gamma\n",
      "   dweibull          -- Double Weibull\n",
      "   erlang            -- Erlang\n",
      "   expon             -- Exponential\n",
      "   exponnorm         -- Exponentially Modified Normal\n",
      "   exponweib         -- Exponentiated Weibull\n",
      "   exponpow          -- Exponential Power\n",
      "   f                 -- F (Snecdor F)\n",
      "   fatiguelife       -- Fatigue Life (Birnbaum-Saunders)\n",
      "   fisk              -- Fisk\n",
      "   foldcauchy        -- Folded Cauchy\n",
      "   foldnorm          -- Folded Normal\n",
      "   frechet_r         -- Deprecated. Alias for weibull_min\n",
      "   frechet_l         -- Deprecated. Alias for weibull_max\n",
      "   genlogistic       -- Generalized Logistic\n",
      "   gennorm           -- Generalized normal\n",
      "   genpareto         -- Generalized Pareto\n",
      "   genexpon          -- Generalized Exponential\n",
      "   genextreme        -- Generalized Extreme Value\n",
      "   gausshyper        -- Gauss Hypergeometric\n",
      "   gamma             -- Gamma\n",
      "   gengamma          -- Generalized gamma\n",
      "   genhalflogistic   -- Generalized Half Logistic\n",
      "   gilbrat           -- Gilbrat\n",
      "   gompertz          -- Gompertz (Truncated Gumbel)\n",
      "   gumbel_r          -- Right Sided Gumbel, Log-Weibull, Fisher-Tippett, Extreme Value Type I\n",
      "   gumbel_l          -- Left Sided Gumbel, etc.\n",
      "   halfcauchy        -- Half Cauchy\n",
      "   halflogistic      -- Half Logistic\n",
      "   halfnorm          -- Half Normal\n",
      "   halfgennorm       -- Generalized Half Normal\n",
      "   hypsecant         -- Hyperbolic Secant\n",
      "   invgamma          -- Inverse Gamma\n",
      "   invgauss          -- Inverse Gaussian\n",
      "   invweibull        -- Inverse Weibull\n",
      "   johnsonsb         -- Johnson SB\n",
      "   johnsonsu         -- Johnson SU\n",
      "   kappa4            -- Kappa 4 parameter\n",
      "   kappa3            -- Kappa 3 parameter\n",
      "   ksone             -- Kolmogorov-Smirnov one-sided (no stats)\n",
      "   kstwobign         -- Kolmogorov-Smirnov two-sided test for Large N (no stats)\n",
      "   laplace           -- Laplace\n",
      "   levy              -- Levy\n",
      "   levy_l\n",
      "   levy_stable\n",
      "   logistic          -- Logistic\n",
      "   loggamma          -- Log-Gamma\n",
      "   loglaplace        -- Log-Laplace (Log Double Exponential)\n",
      "   lognorm           -- Log-Normal\n",
      "   lomax             -- Lomax (Pareto of the second kind)\n",
      "   maxwell           -- Maxwell\n",
      "   mielke            -- Mielke's Beta-Kappa\n",
      "   moyal             -- Moyal\n",
      "   nakagami          -- Nakagami\n",
      "   ncx2              -- Non-central chi-squared\n",
      "   ncf               -- Non-central F\n",
      "   nct               -- Non-central Student's T\n",
      "   norm              -- Normal (Gaussian)\n",
      "   norminvgauss      -- Normal Inverse Gaussian\n",
      "   pareto            -- Pareto\n",
      "   pearson3          -- Pearson type III\n",
      "   powerlaw          -- Power-function\n",
      "   powerlognorm      -- Power log normal\n",
      "   powernorm         -- Power normal\n",
      "   rdist             -- R-distribution\n",
      "   reciprocal        -- Reciprocal\n",
      "   rayleigh          -- Rayleigh\n",
      "   rice              -- Rice\n",
      "   recipinvgauss     -- Reciprocal Inverse Gaussian\n",
      "   semicircular      -- Semicircular\n",
      "   skewnorm          -- Skew normal\n",
      "   t                 -- Student's T\n",
      "   trapz              -- Trapezoidal\n",
      "   triang            -- Triangular\n",
      "   truncexpon        -- Truncated Exponential\n",
      "   truncnorm         -- Truncated Normal\n",
      "   tukeylambda       -- Tukey-Lambda\n",
      "   uniform           -- Uniform\n",
      "   vonmises          -- Von-Mises (Circular)\n",
      "   vonmises_line     -- Von-Mises (Line)\n",
      "   wald              -- Wald\n",
      "   weibull_min       -- Minimum Weibull (see Frechet)\n",
      "   weibull_max       -- Maximum Weibull (see Frechet)\n",
      "   wrapcauchy        -- Wrapped Cauchy\n",
      "\n",
      "Multivariate distributions\n",
      "==========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   multivariate_normal   -- Multivariate normal distribution\n",
      "   matrix_normal         -- Matrix normal distribution\n",
      "   dirichlet             -- Dirichlet\n",
      "   wishart               -- Wishart\n",
      "   invwishart            -- Inverse Wishart\n",
      "   multinomial           -- Multinomial distribution\n",
      "   special_ortho_group   -- SO(N) group\n",
      "   ortho_group           -- O(N) group\n",
      "   unitary_group         -- U(N) group\n",
      "   random_correlation    -- random correlation matrices\n",
      "\n",
      "Discrete distributions\n",
      "======================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   bernoulli         -- Bernoulli\n",
      "   binom             -- Binomial\n",
      "   boltzmann         -- Boltzmann (Truncated Discrete Exponential)\n",
      "   dlaplace          -- Discrete Laplacian\n",
      "   geom              -- Geometric\n",
      "   hypergeom         -- Hypergeometric\n",
      "   logser            -- Logarithmic (Log-Series, Series)\n",
      "   nbinom            -- Negative Binomial\n",
      "   planck            -- Planck (Discrete Exponential)\n",
      "   poisson           -- Poisson\n",
      "   randint           -- Discrete Uniform\n",
      "   skellam           -- Skellam\n",
      "   zipf              -- Zipf\n",
      "   yulesimon         -- Yule-Simon\n",
      "\n",
      "An overview of statistical functions is given below.\n",
      "Several of these functions have a similar version in\n",
      "`scipy.stats.mstats` which work for masked arrays.\n",
      "\n",
      "Summary statistics\n",
      "==================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   describe          -- Descriptive statistics\n",
      "   gmean             -- Geometric mean\n",
      "   hmean             -- Harmonic mean\n",
      "   kurtosis          -- Fisher or Pearson kurtosis\n",
      "   mode              -- Modal value\n",
      "   moment            -- Central moment\n",
      "   skew              -- Skewness\n",
      "   kstat             --\n",
      "   kstatvar          --\n",
      "   tmean             -- Truncated arithmetic mean\n",
      "   tvar              -- Truncated variance\n",
      "   tmin              --\n",
      "   tmax              --\n",
      "   tstd              --\n",
      "   tsem              --\n",
      "   variation         -- Coefficient of variation\n",
      "   find_repeats\n",
      "   trim_mean\n",
      "   gstd              -- Geometric Standard Deviation\n",
      "   iqr\n",
      "   sem\n",
      "   bayes_mvs\n",
      "   mvsdist\n",
      "   entropy\n",
      "   median_absolute_deviation\n",
      "\n",
      "Frequency statistics\n",
      "====================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   cumfreq\n",
      "   itemfreq\n",
      "   percentileofscore\n",
      "   scoreatpercentile\n",
      "   relfreq\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   binned_statistic     -- Compute a binned statistic for a set of data.\n",
      "   binned_statistic_2d  -- Compute a 2-D binned statistic for a set of data.\n",
      "   binned_statistic_dd  -- Compute a d-D binned statistic for a set of data.\n",
      "\n",
      "Correlation functions\n",
      "=====================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   f_oneway\n",
      "   pearsonr\n",
      "   spearmanr\n",
      "   pointbiserialr\n",
      "   kendalltau\n",
      "   weightedtau\n",
      "   linregress\n",
      "   siegelslopes\n",
      "   theilslopes\n",
      "\n",
      "Statistical tests\n",
      "=================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   ttest_1samp\n",
      "   ttest_ind\n",
      "   ttest_ind_from_stats\n",
      "   ttest_rel\n",
      "   kstest\n",
      "   chisquare\n",
      "   power_divergence\n",
      "   ks_2samp\n",
      "   epps_singleton_2samp\n",
      "   mannwhitneyu\n",
      "   tiecorrect\n",
      "   rankdata\n",
      "   ranksums\n",
      "   wilcoxon\n",
      "   kruskal\n",
      "   friedmanchisquare\n",
      "   brunnermunzel\n",
      "   combine_pvalues\n",
      "   jarque_bera\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   ansari\n",
      "   bartlett\n",
      "   levene\n",
      "   shapiro\n",
      "   anderson\n",
      "   anderson_ksamp\n",
      "   binom_test\n",
      "   fligner\n",
      "   median_test\n",
      "   mood\n",
      "   skewtest\n",
      "   kurtosistest\n",
      "   normaltest\n",
      "\n",
      "Transformations\n",
      "===============\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   boxcox\n",
      "   boxcox_normmax\n",
      "   boxcox_llf\n",
      "   yeojohnson\n",
      "   yeojohnson_normmax\n",
      "   yeojohnson_llf\n",
      "   obrientransform\n",
      "   sigmaclip\n",
      "   trimboth\n",
      "   trim1\n",
      "   zmap\n",
      "   zscore\n",
      "\n",
      "Statistical distances\n",
      "=====================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   wasserstein_distance\n",
      "   energy_distance\n",
      "\n",
      "Random variate generation\n",
      "=========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   rvs_ratio_uniforms\n",
      "\n",
      "Circular statistical functions\n",
      "==============================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   circmean\n",
      "   circvar\n",
      "   circstd\n",
      "\n",
      "Contingency table functions\n",
      "===========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   chi2_contingency\n",
      "   contingency.expected_freq\n",
      "   contingency.margins\n",
      "   fisher_exact\n",
      "\n",
      "Plot-tests\n",
      "==========\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   ppcc_max\n",
      "   ppcc_plot\n",
      "   probplot\n",
      "   boxcox_normplot\n",
      "   yeojohnson_normplot\n",
      "\n",
      "\n",
      "Masked statistics functions\n",
      "===========================\n",
      "\n",
      ".. toctree::\n",
      "\n",
      "   stats.mstats\n",
      "\n",
      "\n",
      "Univariate and multivariate kernel density estimation\n",
      "=====================================================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   gaussian_kde\n",
      "\n",
      "Warnings used in :mod:`scipy.stats`\n",
      "===================================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   PearsonRConstantInputWarning\n",
      "   PearsonRNearConstantInputWarning\n",
      "\n",
      "For many more stat related functions install the software R and the\n",
      "interface package rpy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stats.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed this project notebook, you now have good hands-on experience:\n",
    "* using the central limit theorem to help you apply frequentist techniques to answer questions that pertain to very non-normally distributed data from the real world\n",
    "* performing inference using such data to answer business questions\n",
    "* forming a hypothesis and framing the null and alternative hypotheses\n",
    "* testing this using a _t_-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
